{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary libraries for website scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "import ssl\n",
    "import http.client\n",
    "\n",
    "conn = http.client.HTTPSConnection('https://wbsaboojsathi.gov.in/v2/ss_search_student.php', context=ssl.create_default_context(ssl.Purpose.SERVER_AUTH), verify=True)\n",
    "\n",
    "# Make a GET request\n",
    "conn.request('GET', '/')\n",
    "\n",
    "# Get the response\n",
    "response = conn.getresponse()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection timed-out error occurrence due to mutiple operations on the website hence now using a customized ssl with larger DH keys."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the file to get the elements from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parsed_Data(str):\n",
    "    s=requests.get(str)\n",
    "    soup=BeautifulSoup(s.content,\"html.parser\")\n",
    "\n",
    "    return soup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing particular data elements from the parsed and finding the required elements required from the data through its tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1=parsed_Data(str).find(\"div\",class_=\"\")\n",
    "District=DS1.find_all(\"div\",class_=\"\")\n",
    "data=[]\n",
    "for ds in District:\n",
    "    data.append(ds)\n",
    "\n",
    "DS2=parsed_Data(str).find(\"div\",class_=\"\")\n",
    "Block=DS2.find_all('div',class_='')\n",
    "data1=[]\n",
    "for ds1 in Block:\n",
    "    data1.append(ds1)\n",
    "\n",
    "DS3=parsed_Data(str).find(\"div\",class_=\"\")\n",
    "School=DS3.find_all(div='',class_='')\n",
    "data2=[]\n",
    "for ds2 in School:\n",
    "    data2.append(ds2)\n",
    "\n",
    "DS4=parsed_Data(str).find(\"div\",class_=\"\")\n",
    "Phase=DS4.find_all('div',class_='')\n",
    "data3=[]\n",
    "for ds3 in Phase:\n",
    "    data3.append(ds3)\n",
    "\n",
    "DS5=parsed_Data(str).find(\"div\",class_=\"\")\n",
    "Class=DS5.find_all('div',class_='')\n",
    "data4=[]\n",
    "for ds4 in Class:\n",
    "    data4.append(ds4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all the raw data to CSV and Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='District.csv'\n",
    "f=open(filename,'w',newline='')\n",
    "Information_District=csv.writer(f)\n",
    "Information_District.writerow(data) #Writing to CSV\n",
    "x1=[]\n",
    "x1.append(data)\n",
    "df = pd.DataFrame(data = x1[1:],columns = x1[0])\n",
    "df.to_excel('District.xlsx', index=False,header = False)#Writing to Excel file\n",
    "\n",
    "filename1='Block.csv'\n",
    "f1=open(filename,'w',newline='')\n",
    "Information_Block=csv.writer(f1)\n",
    "Information_Block.writerow(data1) #Writing to CSV\n",
    "x2=[]\n",
    "x2.append(data1)\n",
    "df1 = pd.DataFrame(data1 = x2[1:],columns = x2[0])\n",
    "df1.to_excel('Block.xlsx', index=False,header = False)#Writing to Excel file\n",
    "\n",
    "filename2='School.csv'\n",
    "f2=open(filename,'w',newline='')\n",
    "Information_School=csv.writer(f2)\n",
    "Information_School.writerow(data2) #Writing to CSV\n",
    "x3=[]\n",
    "x3.append(data2)\n",
    "df2 = pd.DataFrame(data2 = x3[1:],columns = x3[0])\n",
    "df2.to_excel('School.xlsx', index=False,header = False)#Writing to Excel file\n",
    "\n",
    "filename3='Phase.csv'\n",
    "f3=open(filename,'w',newline='')\n",
    "Information_Phase=csv.writer(f3)\n",
    "Information_Phase.writerow(data) #Writing to CSV\n",
    "x4=[]\n",
    "x4.append(data3)\n",
    "df3 = pd.DataFrame(data3 = x4[1:],columns = x4[0])\n",
    "df3.to_excel('Phase.xlsx', index=False,header = False)#Writing to Excel file\n",
    "\n",
    "filename4='Class.csv'\n",
    "f4=open(filename,'w',newline='')\n",
    "Information_Class=csv.writer(f4)\n",
    "Information_Class.writerow(data) #Writing to CSV\n",
    "x5=[]\n",
    "x5.append(data4)\n",
    "df4 = pd.DataFrame(data4 = x5[1:],columns = x5[0])\n",
    "df4.to_excel('Class.xlsx', index=False,header = False)#Writing to Excel file\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving the URL which is to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=input()\n",
    "parsed_Data(URL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
